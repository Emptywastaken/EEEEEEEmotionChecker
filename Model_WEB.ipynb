{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13cbbcf9-196a-4a57-a545-e40300630c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c3e31e7-391d-40cf-b25c-20d64d9041e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "NOTES:\n",
    "transformations have 2 purposes:\n",
    "1. data augmentation — used during training to improve generalization and reduce overfitting\n",
    "2. standardization — used in all phases (train/val/test) to normalize pixel values and convert to tensors\n",
    "For the first we should use:\n",
    "transforms.ToTensor() - train/test - converts PILImage to tensor, scales to [0,1]\n",
    "For the second we should use:\n",
    "transforms.Normalize(mean, std) - train/test - helps network converge faster, stabilizes training\n",
    "grayscale images - (mean, std) - ((0.5,), (0.5,))\n",
    "color images - (mean, std) - ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "Customize the rest:\n",
    "1. Based on image type\n",
    "grayscale images\t            simple transforms (no color jittering)\n",
    "color images\t                add ColorJitter, RandomGrayscale, etc.\n",
    "medical/scientific\t            avoid strong augmentations\n",
    "natural photos\t                more aggressive augmentation is okay\n",
    "\n",
    "2. Based on image resolution\n",
    "small (e.g. 48x48)\t            avoid aggressive cropping or resizing\n",
    "large (e.g. 224x224 or more)\tyou can use RandomCrop, Resize, etc.\n",
    "\n",
    "3. Based on task type\n",
    "classification\t                horizontal flip, rotation, crop\n",
    "object detection                use bounding-box aware augmentations\n",
    "segmentation\t                need to apply the same transform to mask\n",
    "facial emotion recognition\t    very limited changes — don’t alter facial structure\n",
    "\n",
    "4. Based on dataset size\n",
    "small\t                        apply more augmentation to simulate more data\n",
    "large\t                        minimal augmentation, rely on real variation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class FER2013Dataset:\n",
    "    # constructor method used to initialize the dataset loader class\n",
    "    # params: base_path (path where the dataset is located)\n",
    "    # params: batch_size (number of samples per batch to load during training)\n",
    "    def __init__(self, base_path='/Users/mariakalianova/PycharmProjects/PythonProject/CNN/archive', batch_size=64):\n",
    "        # ------------------------------------------------\n",
    "        # DEFINING MAIN PARAMS\n",
    "        # ------------------------------------------------\n",
    "        # build full paths to the training and test (used as validation) directories\n",
    "        self.train_dir = os.path.join(base_path, 'train')\n",
    "        self.val_dir = os.path.join(base_path, 'test')\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # INTRODUCE TRANSFORMATIONS FOR TEST AND TRAIN DATASET\n",
    "        # ------------------------------------------------\n",
    "        # training transforms include:\n",
    "        # - random horizontal flip (for data augmentation, so it works on 'mirrored' images)\n",
    "        # - random rotation (for robustness to small image rotations)\n",
    "        # - tensor conversion (turns PIL image into torch tensor)\n",
    "        # - normalization (scales pixel values to [-1, 1] range)\n",
    "        self.train_transforms = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  # normalization for grayscale images\n",
    "        ])\n",
    "\n",
    "        # define preprocessing for validation images\n",
    "        # no augmentation is used here to keep evaluation consistent\n",
    "        self.val_transforms = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # CREATE TRANSFORMED DATASETS\n",
    "        # ------------------------------------------------\n",
    "        # create the training and testing dataset using the folder structure and training transforms\n",
    "        self.train_dataset = ImageFolder(root=self.train_dir, transform=self.train_transforms)\n",
    "        self.val_dataset = ImageFolder(root=self.val_dir, transform=self.val_transforms)\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # WRAP TRANSFORMED DATASETS\n",
    "        # ------------------------------------------------\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    # returns training and validation dataloaders\n",
    "    # access to preloaded dataloaders for use in training and evaluation loops\n",
    "    def get_loaders(self):\n",
    "        return self.train_loader, self.val_loader\n",
    "\n",
    "    # returns a list of class names\n",
    "    def get_classes(self):\n",
    "        return self.train_dataset.classes\n",
    "\n",
    "    # returns a dictionary mapping class names to numeric labels\n",
    "    def get_class_to_idx(self):\n",
    "        return self.train_dataset.class_to_idx\n",
    "\n",
    "    # returns number of samples in training dataset\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9286196-360f-4db3-ba59-86b1dc1bb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "dat = FER2013Dataset()\n",
    "print(dat.get_class_to_idx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f637cd8-6f74-4e87-b43b-e656dfd90460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRecognitionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionRecognitionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 1024)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 48 -> 24\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 24 -> 12\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 12 -> 6\n",
    "        x = x.view(-1, 256 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70cf02db-fece-4cb7-8d8e-195bbfbff011",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionRecognitionCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c33660ce-d62b-4450-8f29-2a2b6ae5aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█| 449/449 [01:07<00:00,  6.61batch/s, accuracy=0.339, loss=1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.6540, Accuracy: 0.3389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█| 449/449 [01:06<00:00,  6.76batch/s, accuracy=0.45, loss=1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 1.4331, Accuracy: 0.4502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█| 449/449 [01:05<00:00,  6.80batch/s, accuracy=0.496, loss=1.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 1.3168, Accuracy: 0.4960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█| 449/449 [01:06<00:00,  6.77batch/s, accuracy=0.526, loss=1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 1.2539, Accuracy: 0.5259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█| 449/449 [01:05<00:00,  6.84batch/s, accuracy=0.542, loss=1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 1.2048, Accuracy: 0.5419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█| 449/449 [01:05<00:00,  6.88batch/s, accuracy=0.556, loss=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 1.1648, Accuracy: 0.5563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|█| 449/449 [01:04<00:00,  6.97batch/s, accuracy=0.573, loss=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 1.1228, Accuracy: 0.5734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|█| 449/449 [01:05<00:00,  6.89batch/s, accuracy=0.585, loss=1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 1.0989, Accuracy: 0.5849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|█| 449/449 [01:05<00:00,  6.82batch/s, accuracy=0.593, loss=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 1.0755, Accuracy: 0.5932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█| 449/449 [01:07<00:00,  6.66batch/s, accuracy=0.606, loss=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 1.0424, Accuracy: 0.6058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = FER2013Dataset(base_path=\"/Users/mariakalianova/PycharmProjects/PythonProject/CNN/archive\", batch_size=64)\n",
    "train_loader, val_loader = dataset.get_loaders()\n",
    "\n",
    "# Initialize model\n",
    "model = EmotionRecognitionCNN().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\")\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Accuracy tracking\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            loss=running_loss / len(train_loader),\n",
    "            accuracy=correct_predictions / total_predictions\n",
    "        )\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20873664-b808-4d6f-a336-8a2d73f8a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "val_loss = 0.0\n",
    "val_correct = 0\n",
    "val_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2302b229-082a-4f5a-95f2-9196c3e5d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1968, Validation Accuracy: 0.5893\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        val_correct += (predicted == labels).sum().item()\n",
    "        val_total += labels.size(0)\n",
    "\n",
    "val_accuracy = val_correct / val_total\n",
    "val_loss_avg = val_loss / len(val_loader)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss_avg:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abcfe66d-b294-4f92-ba44-7a54b85a4422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'emotion_model2.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"emotion_model2_58.pth\")\n",
    "print(\"Model saved as 'emotion_model2.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53b69228-55e8-44a3-9c08-f3e8809ef1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionRecognitionCNN(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=1024, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=1024, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmotionRecognitionCNN()\n",
    "model.load_state_dict(torch.load(\"emotion_model2_58.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7c1b782-96e8-4bd0-9ea4-0f4c511706ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your dataset object\n",
    "dataset = FER2013Dataset(base_path=\"/Users/mariakalianova/PycharmProjects/PythonProject/CNN/archive\")\n",
    "\n",
    "# Get mapping: {'angry': 0, ..., 'surprise': 6}\n",
    "class_to_idx = dataset.get_class_to_idx()\n",
    "\n",
    "# Reverse it: {0: 'angry', ..., 6: 'surprise'}\n",
    "emotion_dict = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "# Preprocessing: match training\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "        face_pil = Image.fromarray(face)\n",
    "        face_tensor = preprocess(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(face_tensor)\n",
    "            prediction = torch.argmax(output, 1).item()\n",
    "            emotion = emotion_dict[prediction]\n",
    "\n",
    "        # Draw box and label\n",
    "        cv2.reqctangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, emotion, (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    cv2.imshow('Real-Time Emotion Recognition', frame)\n",
    "\n",
    "    # Exit on 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee164b94-0fa7-4282-843f-56c43b7355ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m----> 5\u001b[0m conf_mat \u001b[38;5;241m=\u001b[39m confusion_matrix(true_labels, predicted_labels)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# confusion matrix\u001b[39;00m\n\u001b[1;32m      8\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "conf_mat = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=train_data.classes, yticklabels=train_data.classes, ax=ax)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373ac3e-df45-4cf7-b131-c4413f93e936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53e6e6-1ea0-4376-b71a-98100be96c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
